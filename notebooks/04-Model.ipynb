{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "PATH_ROOT = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Modeling Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Model Performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data.obtain import get_raw_data\n",
    "from src.data.scrub import scrub_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.models.persist import persist\n",
    "from src.data.obtain import json_write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "plt.style.use('seaborn-white')\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 5 \n",
    "rcParams['font.family'] = 'Roboto'\n",
    "\n",
    "font_title = {\n",
    "    'size': 18, \n",
    "    'weight': \"bold\", \n",
    "    'name': 'Montserrat'\n",
    "}\n",
    "\n",
    "font_axes = {\n",
    "    'size': 14, \n",
    "    'weight': \"bold\", \n",
    "    'name': 'Montserrat'\n",
    "}\n",
    "\n",
    "font_text = {\n",
    "    'size': 14, \n",
    "    'weight': 400, \n",
    "    'name': 'Roboto'\n",
    "}\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving cleaned data from backup.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('data/04-processed/titanic.csv'):\n",
    "    print(\"Importing and scrubbing raw data.\")\n",
    "    df_raw = get_raw_data()\n",
    "    df = scrub_raw_data(df_raw)\n",
    "else:\n",
    "    print(\"Retrieving cleaned data from backup.\")\n",
    "    df = pd.read_csv('data/04-processed/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.copy().drop(['survived'], axis=1).fillna(0)\n",
    "y = df['survived'].copy()\n",
    "\n",
    "# Train-Test Split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "scaler.fit(X_tr)\n",
    "\n",
    "X_tr__scaled = scaler.transform(X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_write(persist(scaler), '/home/src/data/scaler_params.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model object, fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_0 = LogisticRegression()\n",
    "lr_0.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.721374045802\n"
     ]
    }
   ],
   "source": [
    "X_te__scaled = scaler.transform(X_te)\n",
    "\n",
    "y_pr = lr_0.predict(X_te__scaled)\n",
    "print(\"Baseline Accuracy: {}\".format(accuracy_score(y_te, y_pr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without cross-validation, performance metrics arent reliable.\n",
      "0.77\n",
      "0.76\n",
      "0.79\n",
      "0.77\n",
      "0.72\n"
     ]
    }
   ],
   "source": [
    "print(\"Without cross-validation, performance metrics arent reliable.\")\n",
    "for i in range(5):\n",
    "    \"\"\"\n",
    "    Loop to prove the need for cross-validated output\n",
    "    \"\"\"    \n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, train_size=0.8, test_size=0.2)\n",
    "    scaler.fit(X_tr)\n",
    "    \n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_te = scaler.transform(X_te)\n",
    "\n",
    "    lr_0 = LogisticRegression()\n",
    "    lr_0.fit(X_tr, y_tr)\n",
    "    print accuracy_score(lr_0.predict(X_te), y_te).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients: \n",
      "[-0.53, -0.39, -0.02, 0.09, -0.16, -0.07, -0.39, 0.17, 1.26, 0.17, 0.01, -0.15, 0.33, 0.03, -0.32]\n",
      "\n",
      "Model Parameters: \n",
      "{'warm_start': False, 'C': 1.0, 'n_jobs': 1, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 100, 'penalty': 'l2', 'multi_class': 'ovr', 'random_state': None, 'dual': False, 'tol': 0.0001, 'solver': 'liblinear', 'class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Coefficients: \\n{}\".format(lr_0.coef_[0].round(2).tolist()))\n",
    "print\n",
    "print(\"Model Parameters: \\n{}\".format(lr_0.get_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_write(persist(lr_0), '/home/src/data/lr0_params.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.77\n",
      "\n",
      "Best Params: \n",
      "{'model__C': 0.1, 'model__class_weight': 'balanced', 'model__penalty': 'l1'}\n",
      "\n",
      "OOS Accuracy: \n",
      "0.8\n",
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.87      0.82       139\n",
      "          1       0.83      0.72      0.77       123\n",
      "\n",
      "avg / total       0.80      0.80      0.80       262\n",
      "\n",
      "Coefficients: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pclass_3               -0.33\n",
       "sibsp                  -0.29\n",
       "cabinnumber__is_null   -0.29\n",
       "age                    -0.27\n",
       "embarked_S             -0.13\n",
       "age__is_null           -0.06\n",
       "parch                   0.00\n",
       "fare                    0.00\n",
       "fare__is_null           0.00\n",
       "embarked__is_null       0.00\n",
       "embarked_Q              0.00\n",
       "pclass_2                0.00\n",
       "embarked_C              0.08\n",
       "pclass_1                0.22\n",
       "gender                  1.04\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, train_size=0.7, test_size=0.2)\n",
    "\n",
    "# Set up the Pipeline\n",
    "pipe_lr = \\\n",
    "Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Set up the Grid Search\n",
    "grid_lr = {\n",
    "    'model__C': [0.01, 0.1, 1, 10],\n",
    "    'model__penalty':['l1', 'l2'],\n",
    "    'model__class_weight':[None, 'balanced']\n",
    "}\n",
    "\n",
    "# Run the Grid Search\n",
    "gscv_lr = GridSearchCV(\n",
    "    estimator=pipe_lr, \n",
    "    param_grid=grid_lr, \n",
    "    scoring='recall', \n",
    "    cv=5)\n",
    "\n",
    "gscv_lr.fit(X_tr, y_tr)\n",
    "y_pr = gscv_lr.best_estimator_.predict(X_te)\n",
    "\n",
    "print(\"Best score:\\n{}\\n\".format(gscv_lr.best_score_.round(2)))\n",
    "print(\"Best Params: \\n{}\\n\".format(gscv_lr.best_params_))\n",
    "print(\"OOS Accuracy: \\n{}\\n\".format(accuracy_score(y_pr, y_te).round(2)))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_pr, y_te))\n",
    "print(\"Coefficients: \\n\")\n",
    "Series(gscv_lr.best_estimator_.named_steps.get('model').coef_[0], index=X.columns.tolist()).round(2).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the `GridSearchCV` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{sklearn.metrics.scorer._PredictScorer,\n",
       " sklearn.pipeline.Pipeline,\n",
       " numpy.int64,\n",
       " numpy.float64,\n",
       " bool,\n",
       " int,\n",
       " dict,\n",
       " NoneType,\n",
       " str}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{type(v) for k, v in gscv_lr.__dict__.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_estimator_': sklearn.pipeline.Pipeline,\n",
       " 'best_index_': numpy.int64,\n",
       " 'best_params_': dict,\n",
       " 'best_score_': numpy.float64,\n",
       " 'cv': int,\n",
       " 'cv_results_': dict,\n",
       " 'error_score': str,\n",
       " 'estimator': sklearn.pipeline.Pipeline,\n",
       " 'fit_params': NoneType,\n",
       " 'iid': bool,\n",
       " 'multimetric_': bool,\n",
       " 'n_jobs': int,\n",
       " 'n_splits_': int,\n",
       " 'param_grid': dict,\n",
       " 'pre_dispatch': str,\n",
       " 'refit': bool,\n",
       " 'return_train_score': bool,\n",
       " 'scorer_': sklearn.metrics.scorer._PredictScorer,\n",
       " 'scoring': str,\n",
       " 'verbose': int}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:type(v) for k, v in gscv_lr.__dict__.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting a Pipeline Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('scale', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('model', LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "             fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "             multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "             solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_lr.best_estimator_.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_0 = gscv_lr.best_estimator_.__dict__\n",
    "persist_0 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "persist_0['memory'] = pipe_0['memory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale <class 'sklearn.preprocessing.data.StandardScaler'>\n",
      "model <class 'sklearn.linear_model.logistic.LogisticRegression'>\n"
     ]
    }
   ],
   "source": [
    "for s in pipe_0['steps']:\n",
    "    print s[0], type(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator_to_JSON(estimator='', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.models.train import run_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_models = \\\n",
    "map(lambda i:\n",
    "    run_classifier(\n",
    "        X=X, \n",
    "        y=y,\n",
    "        UNCORR=X.columns.tolist(),\n",
    "        TRAIN_SIZE=0.75,\n",
    "        CLF=pipe_lr, \n",
    "        GRID=grid_lr, \n",
    "        SCORING='roc_auc'),\n",
    "    range(10))\n",
    "\n",
    "df_ten_models = \\\n",
    "(pd.concat(\n",
    "    map(lambda model:\n",
    "        Series({k:model[k] for k in model.keys() if 'score' in k}), list_models),\n",
    "    axis=1))\n",
    "\n",
    "df_ten_models.columns = map(lambda i: 'split_{}'.format(i), df_ten_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "(df_ten_models\n",
    " .T\n",
    " .drop(['train_score', 'test_score__accuracy'], axis=1)\n",
    " .plot.box(vert=False, xlim=(0.55, 0.85), ax=ax)\n",
    ");\n",
    "\n",
    "ax.set_title(\"Model Performance over ten random splits\\n\", fontdict=font_title);\n",
    "ax.set_xlabel(\"\\nScore\", fontdict=font_axes)\n",
    "ax.set_ylabel(\"Performance Metric\\n\", fontdict=font_axes)\n",
    "\n",
    "\n",
    "ax.grid(True, linestyle=\":\", alpha=0.6)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.savefig(\"reports/figures/01-model-performance-LR.png\", bbox_inches='tight', pad_inches=0.5)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ten_models.round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choice = int(raw_input(\"Pick a split number from the table above.\\nValid entries (0-9)\\n\"))\n",
    "SELECTED_MODEL = list_models[choice]\n",
    "gscv = SELECTED_MODEL.get('model')\n",
    "\n",
    "print \"GridSearch Results...\\n\"\n",
    "(DataFrame(gscv.cv_results_)\n",
    " .set_index('params')\n",
    " .loc[:, ['mean_train_score', 'mean_test_score', 'mean_fit_time']]\n",
    " .sort_values('mean_test_score', ascending=False)\n",
    " .round(2)\n",
    " .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SELECTED_MODEL.get('model').best_estimator_.named_steps.get('model').get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Series(SELECTED_MODEL.get('model').best_estimator_.named_steps.get('model').coef_[0].round(3),\n",
    "       index=X.columns).round(2).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_dt = \\\n",
    "Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('select', SelectKBest(score_func=f_classif)),\n",
    "    ('model', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "grid_dt = {\n",
    "    'select__k': [5, 9, 'all'],\n",
    "    'model__max_depth':[3, 5, 7],\n",
    "    'model__min_samples_split': [20, 40, 80],\n",
    "    'model__class_weight': ['balanced', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# RUN GRID SEARCHES OVER 10 RANDOM SPLITS\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "list_models = \\\n",
    "map(lambda i:\n",
    "    run_classifier(\n",
    "        X=X, \n",
    "        y=y,\n",
    "        UNCORR=X.columns.tolist(),\n",
    "        TRAIN_SIZE=0.8,\n",
    "        CLF=pipe_dt, \n",
    "        GRID=grid_dt, \n",
    "        SCORING='roc_auc'),\n",
    "    range(10))\n",
    "\n",
    "df_ten_models = \\\n",
    "(pd.concat(\n",
    "    map(lambda model:\n",
    "        Series({k:model[k] for k in model.keys() if 'score' in k}), list_models),\n",
    "    axis=1))\n",
    "\n",
    "df_ten_models.columns = map(lambda i: 'split_{}'.format(i), df_ten_models)\n",
    "df_ten_models.round(2)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PERFORMANCE ANALYSIS\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "(df_ten_models\n",
    " .T\n",
    " .drop(['train_score', 'test_score__accuracy'], axis=1)\n",
    " .plot.box(vert=False, title=\"Model Performance over ten random splits\", xlim=(0, 1))\n",
    ");\n",
    "\n",
    "df_ten_models.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# FINAL MODEL\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "choice = int(raw_input(\"Pick a split number from the table above.\\nValid entries (0-9)\\n\"))\n",
    "SELECTED_MODEL = list_models[choice]\n",
    "gscv = SELECTED_MODEL.get('model')\n",
    "\n",
    "print \"GridSearch Results...\\n\"\n",
    "(DataFrame(gscv.cv_results_)\n",
    " .set_index('params')\n",
    " .loc[:, ['mean_train_score', 'mean_test_score', 'mean_fit_time']]\n",
    " .sort_values('mean_test_score', ascending=False)\n",
    " .round(2)\n",
    " .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SELECTED_MODEL.get('model').best_estimator_.named_steps.get('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Series(SELECTED_MODEL.get('model').best_estimator_.named_steps.get('model').feature_importances_,\n",
    "       index=X.columns.tolist()).round(2).sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
