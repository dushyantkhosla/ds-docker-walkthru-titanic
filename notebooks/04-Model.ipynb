{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "PATH_ROOT = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Modeling Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Model Performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data.obtain import get_raw_data\n",
    "from src.data.scrub import scrub_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.models.persist import persist\n",
    "from src.data.obtain import json_write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "plt.style.use('seaborn-white')\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 5 \n",
    "rcParams['font.family'] = 'Roboto'\n",
    "\n",
    "font_title = {\n",
    "    'size': 18, \n",
    "    'weight': \"bold\", \n",
    "    'name': 'Montserrat'\n",
    "}\n",
    "\n",
    "font_axes = {\n",
    "    'size': 14, \n",
    "    'weight': \"bold\", \n",
    "    'name': 'Montserrat'\n",
    "}\n",
    "\n",
    "font_text = {\n",
    "    'size': 14, \n",
    "    'weight': 400, \n",
    "    'name': 'Roboto'\n",
    "}\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing and scrubbing raw data.\n",
      "Declaring an empty dictionary to persist information we'll need to scrub new data in production.\n",
      "Fixing column names. (Removing special characters, converting to lowercase. Renaming long columns)\n",
      "The following columns have missing data: \n",
      "['age', 'fare', 'cabinnumber', 'embarked']\n",
      "Age is approximately normally distributed, but Fare is skewed.\n",
      "Using the mean for Age and Median for Fare to impute missing data.\n",
      "Cabin Number has over 70% values missing. Dropping this variable.\n",
      "Embarked has only 2 values missing. Imputing with Mode.\n",
      "Creating a column for Gender\n",
      "Creating Dummies for Embarked and Passenger Class. \n",
      "Done. Now dropping these.\n",
      "Dropping cabinnumber, ticket and name as they have no predictive value. (Too many uniques)\n",
      "Downcasting numerics to occupy less space.\n",
      "Backing up the data. \n",
      "Okay, all Done. Happy Exploring!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('data/04-processed/titanic.csv'):\n",
    "    print(\"Importing and scrubbing raw data.\")\n",
    "    df_raw = get_raw_data()\n",
    "    df = scrub_raw_data(df_raw)\n",
    "else:\n",
    "    print(\"Retrieving cleaned data from backup.\")\n",
    "    df = pd.read_csv('data/04-processed/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.copy().drop(['survived'], axis=1).fillna(0)\n",
    "y = df['survived'].copy()\n",
    "\n",
    "# Train-Test Split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "scaler.fit(X_tr)\n",
    "\n",
    "X_tr__scaled = scaler.transform(X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_write(persist(scaler), '/home/src/data/scaler_params.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model object, fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_0 = LogisticRegression()\n",
    "lr_0.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.74427480916\n"
     ]
    }
   ],
   "source": [
    "X_te__scaled = scaler.transform(X_te)\n",
    "\n",
    "y_pr = lr_0.predict(X_te__scaled)\n",
    "print(\"Baseline Accuracy: {}\".format(accuracy_score(y_te, y_pr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without cross-validation, performance metrics arent reliable.\n",
      "0.79\n",
      "0.8\n",
      "0.76\n",
      "0.8\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Without cross-validation, performance metrics arent reliable.\")\n",
    "for i in range(5):\n",
    "    \"\"\"\n",
    "    Loop to prove the need for cross-validated output\n",
    "    \"\"\"    \n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, train_size=0.8, test_size=0.2)\n",
    "    scaler.fit(X_tr)\n",
    "    \n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_te = scaler.transform(X_te)\n",
    "\n",
    "    lr_0 = LogisticRegression()\n",
    "    lr_0.fit(X_tr, y_tr)\n",
    "    print accuracy_score(lr_0.predict(X_te), y_te).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients: \n",
      "[-0.48, -0.32, 0.02, 0.1, -0.07, -0.08, -0.35, 0.17, 1.18, 0.12, 0.01, -0.11, 0.3, 0.1, -0.34]\n",
      "\n",
      "Model Parameters: \n",
      "{'warm_start': False, 'C': 1.0, 'n_jobs': 1, 'verbose': 0, 'intercept_scaling': 1, 'fit_intercept': True, 'max_iter': 100, 'penalty': 'l2', 'multi_class': 'ovr', 'random_state': None, 'dual': False, 'tol': 0.0001, 'solver': 'liblinear', 'class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Coefficients: \\n{}\".format(lr_0.coef_[0].round(2).tolist()))\n",
    "print\n",
    "print(\"Model Parameters: \\n{}\".format(lr_0.get_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_write(persist(lr_0), '/home/src/data/lr0_params.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X_scaled, y, train_size=0.7, test_size=0.2)\n",
    "\n",
    "# Set up the Pipeline\n",
    "pipe_lr = \\\n",
    "    Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('model', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "# Set up the Grid Search\n",
    "grid_lr = {\n",
    "    'model__C': [0.01, 0.1, 1, 10],\n",
    "    'model__penalty':['l1', 'l2'],\n",
    "    'model__class_weight':[None, 'balanced']\n",
    "}\n",
    "\n",
    "# Run the Grid Search\n",
    "gscv_lr = GridSearchCV(\n",
    "    estimator=pipe_lr, \n",
    "    param_grid=grid_lr, \n",
    "    scoring='recall', \n",
    "    cv=5)\n",
    "\n",
    "gscv_lr.fit(X_tr, y_tr)\n",
    "y_pr = gscv_lr.best_estimator_.predict(X_te)\n",
    "\n",
    "print(\"Best score:\\n{}\\n\".format(gscv_lr.best_score_.round(2)))\n",
    "print(\"Best Params: \\n{}\\n\".format(gscv_lr.best_params_))\n",
    "print(\"OOS Accuracy: \\n{}\\n\".format(accuracy_score(y_pr, y_te).round(2)))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_pr, y_te))\n",
    "print(\"Coefficients: \\n\")\n",
    "Series(gscv_lr.best_estimator_.named_steps.get('model').coef_[0], index=X.columns.tolist()).round(2).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.models.train import run_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_models = \\\n",
    "map(lambda i:\n",
    "    run_classifier(\n",
    "        X=X, \n",
    "        y=y,\n",
    "        UNCORR=X.columns.tolist(),\n",
    "        TRAIN_SIZE=0.75,\n",
    "        CLF=pipe_lr, \n",
    "        GRID=grid_lr, \n",
    "        SCORING='roc_auc'),\n",
    "    range(10))\n",
    "\n",
    "df_ten_models = \\\n",
    "(pd.concat(\n",
    "    map(lambda model:\n",
    "        Series({k:model[k] for k in model.keys() if 'score' in k}), list_models),\n",
    "    axis=1))\n",
    "\n",
    "df_ten_models.columns = map(lambda i: 'split_{}'.format(i), df_ten_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "(df_ten_models\n",
    " .T\n",
    " .drop(['train_score', 'test_score__accuracy'], axis=1)\n",
    " .plot.box(vert=False, xlim=(0.55, 0.85), ax=ax)\n",
    ");\n",
    "\n",
    "ax.set_title(\"Model Performance over ten random splits\\n\", fontdict=font_title);\n",
    "ax.set_xlabel(\"\\nScore\", fontdict=font_axes)\n",
    "ax.set_ylabel(\"Performance Metric\\n\", fontdict=font_axes)\n",
    "\n",
    "\n",
    "ax.grid(True, linestyle=\":\", alpha=0.6)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.savefig(\"reports/figures/01-model-performance-LR.png\", bbox_inches='tight', pad_inches=0.5)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ten_models.round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choice = int(raw_input(\"Pick a split number from the table above.\\nValid entries (0-9)\\n\"))\n",
    "SELECTED_MODEL = list_models[choice]\n",
    "gscv = SELECTED_MODEL.get('model')\n",
    "\n",
    "print \"GridSearch Results...\\n\"\n",
    "(DataFrame(gscv.cv_results_)\n",
    " .set_index('params')\n",
    " .loc[:, ['mean_train_score', 'mean_test_score', 'mean_fit_time']]\n",
    " .sort_values('mean_test_score', ascending=False)\n",
    " .round(2)\n",
    " .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SELECTED_MODEL.get('model').best_estimator_.named_steps.get('model').get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Series(SELECTED_MODEL.get('model').best_estimator_.named_steps.get('model').coef_[0].round(3),\n",
    "       index=X.columns).round(2).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_dt = \\\n",
    "Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('select', SelectKBest(score_func=f_classif)),\n",
    "    ('model', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "grid_dt = {\n",
    "    'select__k': [5, 9, 'all'],\n",
    "    'model__max_depth':[3, 5, 7],\n",
    "    'model__min_samples_split': [20, 40, 80],\n",
    "    'model__class_weight': ['balanced', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# RUN GRID SEARCHES OVER 10 RANDOM SPLITS\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "list_models = \\\n",
    "map(lambda i:\n",
    "    run_classifier(\n",
    "        X=X, \n",
    "        y=y,\n",
    "        UNCORR=X.columns.tolist(),\n",
    "        TRAIN_SIZE=0.8,\n",
    "        CLF=pipe_dt, \n",
    "        GRID=grid_dt, \n",
    "        SCORING='roc_auc'),\n",
    "    range(10))\n",
    "\n",
    "df_ten_models = \\\n",
    "(pd.concat(\n",
    "    map(lambda model:\n",
    "        Series({k:model[k] for k in model.keys() if 'score' in k}), list_models),\n",
    "    axis=1))\n",
    "\n",
    "df_ten_models.columns = map(lambda i: 'split_{}'.format(i), df_ten_models)\n",
    "df_ten_models.round(2)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PERFORMANCE ANALYSIS\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "(df_ten_models\n",
    " .T\n",
    " .drop(['train_score', 'test_score__accuracy'], axis=1)\n",
    " .plot.box(vert=False, title=\"Model Performance over ten random splits\", xlim=(0, 1))\n",
    ");\n",
    "\n",
    "df_ten_models.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# FINAL MODEL\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "choice = int(raw_input(\"Pick a split number from the table above.\\nValid entries (0-9)\\n\"))\n",
    "SELECTED_MODEL = list_models[choice]\n",
    "gscv = SELECTED_MODEL.get('model')\n",
    "\n",
    "print \"GridSearch Results...\\n\"\n",
    "(DataFrame(gscv.cv_results_)\n",
    " .set_index('params')\n",
    " .loc[:, ['mean_train_score', 'mean_test_score', 'mean_fit_time']]\n",
    " .sort_values('mean_test_score', ascending=False)\n",
    " .round(2)\n",
    " .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SELECTED_MODEL.get('model').best_estimator_.named_steps.get('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Series(SELECTED_MODEL.get('model').best_estimator_.named_steps.get('model').feature_importances_,\n",
    "       index=X.columns.tolist()).round(2).sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
